{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a481b689-d363-40a9-a8a4-7d8c6a3a67bd",
   "metadata": {},
   "source": [
    "# Benchmarking RAG Pipelines With A `LabelledRagDatatset`\n",
    "\n",
    "The `LabelledRagDataset` is meant to be used for evaluating any given RAG pipeline, for which there could be several configurations (i.e. choosing the `LLM`, values for the `similarity_top_k`, `chunk_size`, and others). We've likened this abstract to traditional machine learning datastets, where `X` features are meant to predict a ground-truth label `y`. In this case, we use the `query` as well as the retrieved `contexts` as the \"features\" and the answer to the query, called `reference_answer` as the ground-truth label.\n",
    "\n",
    "And of course, such datasets are comprised of observations or examples. In the case of `LabelledRagDataset`, these are made up with a set of `LabelledRagDataExample`'s."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9d14822-2779-4da9-bfb7-201f361b8eb1",
   "metadata": {},
   "source": [
    "### The `LabelledRagDataExample` Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5017a52c-e61b-4172-b996-c7d7ce56c825",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llama_dataset import LabelledRagDataExample, CreatedByType\n",
    "\n",
    "# constructing a LabelledRagDataExample\n",
    "query = \"This is a test query, is it not?\"\n",
    "query_by = CreatedByType.AI\n",
    "reference_answer = \"Yes it is.\"\n",
    "reference_answer_by = CreatedByType.HUMAN\n",
    "reference_contexts = [\"This is a sample context\"]\n",
    "\n",
    "rag_example = LabelledRagDataExample(\n",
    "    query=query,\n",
    "    query_by=query_by,\n",
    "    reference_contexts=reference_contexts,\n",
    "    reference_answer=reference_answer,\n",
    "    reference_answer_by=reference_answer_by,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f295953f-bbe1-4cc2-8a60-1eb75678d57e",
   "metadata": {},
   "source": [
    "The `LabelledRagDataExample` is a `dataclasss` and so, going from `json` or `dict` (and vice-versa) is possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bffa5b37-4453-49c4-8527-9a5c772dd436",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"query\": \"This is a test query, is it not?\", \"query_by\": \"ai\", \"reference_contexts\": [\"This is a sample context\"], \"reference_answer\": \"Yes it is.\", \"reference_answer_by\": \"human\"}\n"
     ]
    }
   ],
   "source": [
    "print(rag_example.to_json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c86da0f2-c11b-41b1-bfe7-8c5be9f51a0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LabelledRagDataExample(query='This is a test query, is it not?', query_by=<CreatedByType.AI: 'ai'>, reference_contexts=['This is a sample context'], reference_answer='Yes it is.', reference_answer_by=<CreatedByType.HUMAN: 'human'>)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LabelledRagDataExample.from_json(rag_example.to_json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a23231be-6cc1-49f7-81e9-b2721d1ac836",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': 'This is a test query, is it not?',\n",
       " 'query_by': <CreatedByType.AI: 'ai'>,\n",
       " 'reference_contexts': ['This is a sample context'],\n",
       " 'reference_answer': 'Yes it is.',\n",
       " 'reference_answer_by': <CreatedByType.HUMAN: 'human'>}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_example.to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3040e95-d459-4193-8c45-5238001f1da1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LabelledRagDataExample(query='This is a test query, is it not?', query_by=<CreatedByType.AI: 'ai'>, reference_contexts=['This is a sample context'], reference_answer='Yes it is.', reference_answer_by=<CreatedByType.HUMAN: 'human'>)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LabelledRagDataExample.from_dict(rag_example.to_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97213dd9-c828-4781-af09-c690d9234103",
   "metadata": {},
   "source": [
    "Let's create a second example, so we can have a (slightly) more interesting `LabelledRagDataset`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1986f16f-3f80-4c22-89a0-4d9fa1475d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"This is a test query, is it so?\"\n",
    "reference_answer = \"I think yes, it is.\"\n",
    "reference_contexts = [\"This is a second sample context\"]\n",
    "\n",
    "rag_example_2 = LabelledRagDataExample(\n",
    "    query=query,\n",
    "    query_by=query_by,\n",
    "    reference_contexts=reference_contexts,\n",
    "    reference_answer=reference_answer,\n",
    "    reference_answer_by=reference_answer_by,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "709422c5-8d1d-462a-bfe8-2eabc73c077f",
   "metadata": {},
   "source": [
    "### The `LabelledRagDataset` Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2750320c-ae60-455b-a79c-e8774bf4fc89",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llama_dataset.rag import LabelledRagDataset\n",
    "\n",
    "rag_dataset = LabelledRagDataset(examples=[rag_example, rag_example_2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2812220-f176-49bd-af7c-9a01c3a2dd2a",
   "metadata": {},
   "source": [
    "There exists a convienience method to view the dataset as a `pandas.DataFrame`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1511ddc-c0ed-4aeb-9ff8-76b090d54dde",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query</th>\n",
       "      <th>reference_contexts</th>\n",
       "      <th>reference_answer</th>\n",
       "      <th>reference_answer_by</th>\n",
       "      <th>query_by</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>This is a test query, is it not?</td>\n",
       "      <td>[This is a sample context]</td>\n",
       "      <td>Yes it is.</td>\n",
       "      <td>human</td>\n",
       "      <td>ai</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>This is a test query, is it so?</td>\n",
       "      <td>[This is a second sample context]</td>\n",
       "      <td>I think yes, it is.</td>\n",
       "      <td>human</td>\n",
       "      <td>ai</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              query                 reference_contexts  \\\n",
       "0  This is a test query, is it not?         [This is a sample context]   \n",
       "1   This is a test query, is it so?  [This is a second sample context]   \n",
       "\n",
       "      reference_answer reference_answer_by query_by  \n",
       "0           Yes it is.               human       ai  \n",
       "1  I think yes, it is.               human       ai  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_dataset.to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f541811-043f-4065-9905-0734173f329f",
   "metadata": {},
   "source": [
    "#### Serialization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b63716f0-7723-4f21-9f80-703b257f8b48",
   "metadata": {},
   "source": [
    "To persist and load the dataset to and from disk, there are the `save_json` and `from_json` methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b3a2781-b4c4-49e6-8245-e2381aacc833",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_dataset.save_json(\"rag_dataset.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6400b2c3-5b49-40c2-897b-ac3819beb87a",
   "metadata": {},
   "outputs": [],
   "source": [
    "reload_rag_dataset = LabelledRagDataset.from_json(\"rag_dataset.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "111e13eb-120a-434c-a04b-5c0b9fdcd60f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query</th>\n",
       "      <th>reference_contexts</th>\n",
       "      <th>reference_answer</th>\n",
       "      <th>reference_answer_by</th>\n",
       "      <th>query_by</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>This is a test query, is it not?</td>\n",
       "      <td>[This is a sample context]</td>\n",
       "      <td>Yes it is.</td>\n",
       "      <td>human</td>\n",
       "      <td>ai</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>This is a test query, is it so?</td>\n",
       "      <td>[This is a second sample context]</td>\n",
       "      <td>I think yes, it is.</td>\n",
       "      <td>human</td>\n",
       "      <td>ai</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              query                 reference_contexts  \\\n",
       "0  This is a test query, is it not?         [This is a sample context]   \n",
       "1   This is a test query, is it so?  [This is a second sample context]   \n",
       "\n",
       "      reference_answer reference_answer_by query_by  \n",
       "0           Yes it is.               human       ai  \n",
       "1  I think yes, it is.               human       ai  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reload_rag_dataset.to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "166f98e8-239a-4007-8754-5a9b4edb41a4",
   "metadata": {},
   "source": [
    "### Predicting and Evaluation\n",
    "\n",
    "For this section, we'll first create a `LabelledRagDataset` using a synthetic generator. Ultimately, we will use GPT-4 to produce both the `query` and `reference_answer` for the synthetic `LabelledRagDataExample`'s.\n",
    "\n",
    "NOTE: if one has queries, reference answers, and contexts over a text corpus, then it is not necessary to use data synthesis to be able to predict and subsequently evaluate said predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63cc6fc1-376c-44ca-8370-a0849e96265e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "133efdb3-cfdb-49db-8b9a-76338b907500",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import VectorStoreIndex, SimpleDirectoryReader\n",
    "\n",
    "# Load documents and build index\n",
    "documents = SimpleDirectoryReader(\n",
    "    input_files=[\"data/paul_graham_essay_truncated.txt\"]\n",
    ").load_data()\n",
    "index = VectorStoreIndex.from_documents(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "160c6567-b420-4d5c-a022-dfb4889da795",
   "metadata": {},
   "source": [
    "The `RagDatasetGenerator` can be build over a set of documents to generate `LabelledRagDataExample`'s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f930d75-7a93-4d4c-a647-5f8eeefe37ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate questions against chunks\n",
    "from llama_index.llama_dataset.generator import RagDatasetGenerator\n",
    "from llama_index.llms import OpenAI\n",
    "from llama_index import ServiceContext\n",
    "\n",
    "# set context for llm provider\n",
    "gpt_35_context = ServiceContext.from_defaults(\n",
    "    llm=OpenAI(model=\"gpt-3.5-turbo\", temperature=0.3)\n",
    ")\n",
    "\n",
    "# instantiate a DatasetGenerator\n",
    "dataset_generator = RagDatasetGenerator.from_documents(\n",
    "    documents,\n",
    "    service_context=gpt_35_context,\n",
    "    num_questions_per_chunk=2,  # set the number of questions per nodes\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf592778-a914-4bbf-a88b-3bbc715d70d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset_generator.nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "400c3dc4-3109-411c-87eb-2d130ff757a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# since there are 2 nodes, there should be a total of 4 questions\n",
    "rag_dataset = dataset_generator.generate_dataset_from_nodes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2052a43e-3a7a-4f0f-9f0c-7542ffbb64d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query</th>\n",
       "      <th>reference_contexts</th>\n",
       "      <th>reference_answer</th>\n",
       "      <th>reference_answer_by</th>\n",
       "      <th>query_by</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What were the two main things that the author ...</td>\n",
       "      <td>[What I Worked On\\n\\nFebruary 2021\\n\\nBefore c...</td>\n",
       "      <td>Before college, the two main things that the a...</td>\n",
       "      <td>ai</td>\n",
       "      <td>ai</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What factors influenced the author's decision ...</td>\n",
       "      <td>[What I Worked On\\n\\nFebruary 2021\\n\\nBefore c...</td>\n",
       "      <td>The factors that influenced the author's decis...</td>\n",
       "      <td>ai</td>\n",
       "      <td>ai</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>In the context of the given information, what ...</td>\n",
       "      <td>[I couldn't have put this into words when I wa...</td>\n",
       "      <td>The two factors that influenced the author's d...</td>\n",
       "      <td>ai</td>\n",
       "      <td>ai</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>How did learning Lisp expand the author's conc...</td>\n",
       "      <td>[I couldn't have put this into words when I wa...</td>\n",
       "      <td>Learning Lisp expanded the author's concept of...</td>\n",
       "      <td>ai</td>\n",
       "      <td>ai</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               query  \\\n",
       "0  What were the two main things that the author ...   \n",
       "1  What factors influenced the author's decision ...   \n",
       "2  In the context of the given information, what ...   \n",
       "3  How did learning Lisp expand the author's conc...   \n",
       "\n",
       "                                  reference_contexts  \\\n",
       "0  [What I Worked On\\n\\nFebruary 2021\\n\\nBefore c...   \n",
       "1  [What I Worked On\\n\\nFebruary 2021\\n\\nBefore c...   \n",
       "2  [I couldn't have put this into words when I wa...   \n",
       "3  [I couldn't have put this into words when I wa...   \n",
       "\n",
       "                                    reference_answer reference_answer_by  \\\n",
       "0  Before college, the two main things that the a...                  ai   \n",
       "1  The factors that influenced the author's decis...                  ai   \n",
       "2  The two factors that influenced the author's d...                  ai   \n",
       "3  Learning Lisp expanded the author's concept of...                  ai   \n",
       "\n",
       "  query_by  \n",
       "0       ai  \n",
       "1       ai  \n",
       "2       ai  \n",
       "3       ai  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_dataset.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f86d33ec-61b7-49a9-ab29-050a82088e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_dataset.save_json(\"rag_dataset.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f600662d-d2f6-45fa-8296-db668d7a65cc",
   "metadata": {},
   "source": [
    "#### Predicting\n",
    "\n",
    "Stepping back for a second to paint the situation before moving on to making actual predictions. Recall that the point of the `LabelledRagDataset` is to benchmark any given RAG pipeline that is built over the same source documents (in this case, the `paul_graham_essay_truncated.txt`).\n",
    "\n",
    "So, let's emulate that situation now by creating a simple RAG pipeline (i.e., index, then query engine) over the same source text data file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d24af81-a4ad-4d6b-ae47-4ecc538a3b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = SimpleDirectoryReader(\n",
    "    input_files=[\"data/paul_graham_essay_truncated.txt\"]\n",
    ").load_data()\n",
    "index = VectorStoreIndex.from_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a54c32a-faf4-4642-9541-b1dc12a90e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine = index.as_query_engine()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c26ea016-ef6b-4870-97e7-1e0ac11825f4",
   "metadata": {},
   "source": [
    "A `LabelledRagDataset` has a method call `make_predictions_with` that takes as input a `QueryEngine` to produce predictions (i.e. generate responses to the queries). Specifically, it returns a `RagPredictionDataset` that is comprised of a set of `RagExamplePrediction`'s, which store the generated response as well as the context that was retrieved by the retrievor of the RAG pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e447472f-1f39-46e8-917b-e7e370f5ddd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_dataset = rag_dataset.make_predictions_with(\n",
    "    query_engine=query_engine\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08d2d585-0453-47ac-b6be-8fc8010a8bff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FIRST 100 CHARS of RESPONSE:\n",
      "The author worked on writing and programming before college. In terms of their outcomes, the author ...\n",
      "\n",
      "=================\n",
      "TOP 0 RETRIEVAL:\n",
      "What I Worked On\n",
      "\n",
      "February 2021\n",
      "\n",
      "Before college the two main things I worked on, outside of school, ...\n",
      "\n",
      "=================\n",
      "TOP 1 RETRIEVAL:\n",
      "I couldn't have put this into words when I was 18. All I knew at the time was that I kept taking phi...\n",
      "\n",
      "=================\n"
     ]
    }
   ],
   "source": [
    "# taking a peak at a single RagExamplePrediction\n",
    "pred = prediction_dataset.predictions[0]\n",
    "\n",
    "print(f\"FIRST 100 CHARS of RESPONSE:\\n{pred.response[:100]}...\")\n",
    "print(\"\\n=================\")\n",
    "for ix, c in enumerate(pred.contexts):\n",
    "    print(f\"TOP {ix} RETRIEVAL:\\n{c[:100]}...\\n\")\n",
    "    print(\"=================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81ea390b-4526-4bad-84c8-850df720271a",
   "metadata": {},
   "source": [
    "Just as with `LabelledRagDataset`'s, you can store into and upload from a json."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "878afe82-35ec-43df-98bf-769636d20674",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_dataset.save_json(\"prediction_dataset.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b5461ba-8284-4a93-86c1-8473d5428bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llama_dataset import RagPredictionDataset\n",
    "\n",
    "reloaded_predictions = RagPredictionDataset.from_json(\n",
    "    \"prediction_dataset.json\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b121c1c8-1d21-4442-96dc-5af8195e3443",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>response</th>\n",
       "      <th>contexts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The author worked on writing and programming b...</td>\n",
       "      <td>[What I Worked On\\n\\nFebruary 2021\\n\\nBefore c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The author's decision to switch from studying ...</td>\n",
       "      <td>[I couldn't have put this into words when I wa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The two factors that influenced the author's d...</td>\n",
       "      <td>[I couldn't have put this into words when I wa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Learning Lisp expanded the author's concept of...</td>\n",
       "      <td>[I couldn't have put this into words when I wa...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            response  \\\n",
       "0  The author worked on writing and programming b...   \n",
       "1  The author's decision to switch from studying ...   \n",
       "2  The two factors that influenced the author's d...   \n",
       "3  Learning Lisp expanded the author's concept of...   \n",
       "\n",
       "                                            contexts  \n",
       "0  [What I Worked On\\n\\nFebruary 2021\\n\\nBefore c...  \n",
       "1  [I couldn't have put this into words when I wa...  \n",
       "2  [I couldn't have put this into words when I wa...  \n",
       "3  [I couldn't have put this into words when I wa...  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reloaded_predictions.to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d19d5d9a-d973-4964-86a4-2f57aac96482",
   "metadata": {},
   "source": [
    "#### Evaluation\n",
    "\n",
    "Now that we have our predictions, we can perform evaluations on two dimensions:\n",
    "\n",
    "1. The generated response: how well the predicted response matches the reference answer.\n",
    "2. The retrieved contexts: how well the retrieved contexts for the prediction match the reference contexts.\n",
    "\n",
    "NOTE: For retrieved contexts, we are unable to use standard retrieval metrics such as `hit rate` and `mean reciproccal rank` due to the fact that doing so requires we have the same index that was used to generate the ground truth data. But, it is not necessary for a `LabelledRagDataset` to be even created by an index. As such, we will use `semantic similarity` between the prediction's contexts and the reference contexts as a measure of goodness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8643a3a-b726-4dd2-804e-91998f7bc8c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f76949c6-a7ca-4bf6-ae09-89535adec80e",
   "metadata": {},
   "source": [
    "For evaluating the response, we will use the LLM-As-A-Judge pattern. Specifically, we will use `CorrectnessEvaluator`, `FaithfulnessEvaluator` and `RelevancyEvaluator`.\n",
    "\n",
    "For evaluating the goodness of the retrieved contexts we will use `SemanticSimilarityEvaluator`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03f665dc-ceea-4138-8ef1-b63b8179cef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate the gpt-4 judge\n",
    "from llama_index.llms import OpenAI\n",
    "from llama_index import ServiceContext\n",
    "from llama_index.evaluation import (\n",
    "    CorrectnessEvaluator,\n",
    "    FaithfulnessEvaluator,\n",
    "    RelevancyEvaluator,\n",
    "    SemanticSimilarityEvaluator,\n",
    ")\n",
    "\n",
    "judges = {}\n",
    "\n",
    "judges[\"correctness\"] = CorrectnessEvaluator(\n",
    "    service_context=ServiceContext.from_defaults(\n",
    "        llm=OpenAI(temperature=0, model=\"gpt-4\"),\n",
    "    )\n",
    ")\n",
    "\n",
    "judges[\"relevancy\"] = RelevancyEvaluator(\n",
    "    service_context=ServiceContext.from_defaults(\n",
    "        llm=OpenAI(temperature=0, model=\"gpt-4\"),\n",
    "    )\n",
    ")\n",
    "\n",
    "judges[\"faithfulness\"] = FaithfulnessEvaluator(\n",
    "    service_context=ServiceContext.from_defaults(\n",
    "        llm=OpenAI(temperature=0, model=\"gpt-4\"),\n",
    "    )\n",
    ")\n",
    "\n",
    "judges[\"semantic_similarity\"] = SemanticSimilarityEvaluator(\n",
    "    service_context=ServiceContext.from_defaults(\n",
    "        llm=OpenAI(temperature=0, model=\"gpt-4\"),\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2feb3042-6b06-409b-86ea-581500d35ff5",
   "metadata": {},
   "source": [
    "Loop through the (`labelled_example`, `prediction`) pais and perform the evaluations on each of them individually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8edf32c2-efe2-4ccb-b9b9-a441a1e317d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [00:33,  8.36s/it]\n"
     ]
    }
   ],
   "source": [
    "evals = {\n",
    "    \"correctness\": [],\n",
    "    \"relevancy\": [],\n",
    "    \"faithfulness\": [],\n",
    "    \"context_similarity\": [],\n",
    "}\n",
    "\n",
    "for example, prediction in tqdm.tqdm(\n",
    "    zip(rag_dataset.examples, prediction_dataset.predictions)\n",
    "):\n",
    "    correctness_result = await judges[\"correctness\"].aevaluate(\n",
    "        query=example.query,\n",
    "        response=prediction.response,\n",
    "        reference=example.reference_answer,\n",
    "    )\n",
    "\n",
    "    relevancy_result = judges[\"relevancy\"].evaluate(\n",
    "        query=example.query,\n",
    "        response=prediction.response,\n",
    "        contexts=prediction.contexts,\n",
    "    )\n",
    "\n",
    "    faithfulness_result = judges[\"faithfulness\"].evaluate(\n",
    "        query=example.query,\n",
    "        response=prediction.response,\n",
    "        contexts=prediction.contexts,\n",
    "    )\n",
    "\n",
    "    semantic_similarity_result = judges[\"semantic_similarity\"].evaluate(\n",
    "        query=example.query,\n",
    "        response=\"\\n\".join(prediction.contexts),\n",
    "        reference=\"\\n\".join(example.reference_contexts),\n",
    "    )\n",
    "\n",
    "    evals[\"correctness\"].append(correctness_result)\n",
    "    evals[\"relevancy\"].append(relevancy_result)\n",
    "    evals[\"faithfulness\"].append(faithfulness_result)\n",
    "    evals[\"context_similarity\"].append(semantic_similarity_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6899a20-10d0-4c1a-9403-b5ae811dcde6",
   "metadata": {},
   "source": [
    "Now, we can use our notebook utility functions to view these evaluations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca9c62bb-d2c7-4f5d-a491-8dd25614baf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from llama_index.evaluation.notebook_utils import (\n",
    "    get_eval_results_df,\n",
    ")\n",
    "\n",
    "deep_eval_df, mean_correctness_df = get_eval_results_df(\n",
    "    [\"base_rag\"] * len(evals[\"correctness\"]),\n",
    "    evals[\"correctness\"],\n",
    "    metric=\"correctness\",\n",
    ")\n",
    "deep_eval_df, mean_relevancy_df = get_eval_results_df(\n",
    "    [\"base_rag\"] * len(evals[\"relevancy\"]),\n",
    "    evals[\"relevancy\"],\n",
    "    metric=\"relevancy\",\n",
    ")\n",
    "_, mean_faithfulness_df = get_eval_results_df(\n",
    "    [\"base_rag\"] * len(evals[\"faithfulness\"]),\n",
    "    evals[\"faithfulness\"],\n",
    "    metric=\"faithfulness\",\n",
    ")\n",
    "_, mean_context_similarity_df = get_eval_results_df(\n",
    "    [\"base_rag\"] * len(evals[\"context_similarity\"]),\n",
    "    evals[\"context_similarity\"],\n",
    "    metric=\"context_similarity\",\n",
    ")\n",
    "\n",
    "mean_scores_df = pd.concat(\n",
    "    [\n",
    "        mean_correctness_df.reset_index(),\n",
    "        mean_relevancy_df.reset_index(),\n",
    "        mean_faithfulness_df.reset_index(),\n",
    "        mean_context_similarity_df.reset_index(),\n",
    "    ],\n",
    "    axis=0,\n",
    "    ignore_index=True,\n",
    ")\n",
    "mean_scores_df = mean_scores_df.set_index(\"index\")\n",
    "mean_scores_df.index = mean_scores_df.index.set_names([\"metrics\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae2bf5d8-dafa-4674-86e8-2c04ba721a92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>rag</th>\n",
       "      <th>base_rag</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>metrics</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>mean_correctness_score</th>\n",
       "      <td>4.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean_relevancy_score</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean_faithfulness_score</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean_context_similarity_score</th>\n",
       "      <td>0.974206</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "rag                            base_rag\n",
       "metrics                                \n",
       "mean_correctness_score         4.750000\n",
       "mean_relevancy_score           1.000000\n",
       "mean_faithfulness_score        1.000000\n",
       "mean_context_similarity_score  0.974206"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_scores_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53171536-7562-4cec-ad79-fa091939b0a1",
   "metadata": {},
   "source": [
    "On this toy example, we see that the basic RAG pipeline performs quite well against the evaluation benchmark (`rag_dataset`)!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama_index_3.10",
   "language": "python",
   "name": "llama_index_3.10"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
